{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36aba985",
   "metadata": {},
   "source": [
    "# Overcooked Tutorial\n",
    "This Notebook will demonstrate a couple of common use cases of the Overcooked-AI library, including loading and evaluating agents and visualizing trajectories.\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "ca4bad07",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T12:53:28.069252Z",
     "start_time": "2025-04-08T12:53:21.545161Z"
    }
   },
   "source": [
    "from overcooked_ai_py.agents.agent import AgentPair, RandomAgent\n",
    "from overcooked_ai_py.agents.benchmarking import AgentEvaluator\n",
    "from overcooked_ai_py.visualization.state_visualizer import StateVisualizer\n",
    "\n",
    "# Here we create an evaluator for the cramped_room layout\n",
    "layout = \"cramped_room\"\n",
    "ae = AgentEvaluator.from_layout_name(mdp_params={\"layout_name\": layout, \"old_dynamics\": True}, \n",
    "                                     env_params={\"horizon\": 400})\n",
    "\n",
    "ap = AgentPair(RandomAgent(), RandomAgent())\n",
    "\n",
    "trajs = ae.evaluate_agent_pair(ap, 10)\n",
    "\n",
    "trajs2 = ae.evaluate_human_model_pair(1)\n",
    "\n",
    "\n",
    "StateVisualizer().display_rendered_trajectory(trajs2, ipython_display=True)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Avg rew: 0.00 (std: 0.00, se: 0.00); avg len: 400.00; : 100%|██████████| 10/10 [00:00<00:00, 42.93it/s]\n",
      "Avg rew: 0.00 (std: 0.00, se: 0.00); avg len: 400.00; : 100%|██████████| 1/1 [00:00<00:00, 18.90it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='timestep', max=399), Output()), _dom_classes=('widget-in…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cceb10a8d82f4a8388c2096d774e4e8d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "927f70d1",
   "metadata": {},
   "source": [
    "# Deprecated stuff which requires BC and RL training (see README for details)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca6b8ba",
   "metadata": {},
   "source": [
    "# Getting started: Training your agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a8f96f8",
   "metadata": {},
   "source": [
    "You can train BC agents using files under the `human_aware_rl/imitation` directory. "
   ]
  },
  {
   "cell_type": "code",
   "id": "7f493c88",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T14:09:02.296165Z",
     "start_time": "2025-04-04T14:09:02.174930Z"
    }
   },
   "source": [
    "layout = \"cramped_room\" # any compatible layouts\n",
    "from human_aware_rl.imitation.behavior_cloning_tf2 import get_bc_params, train_bc_model\n",
    "from human_aware_rl.static import CLEAN_2019_HUMAN_DATA_TRAIN\n",
    "\n",
    "params_to_override = {\n",
    "    # this is the layouts where the training will happen\n",
    "    \"layouts\": [layout], \n",
    "    # this is the layout that the agents will be evaluated on\n",
    "    # Most of the time they should be the same, but because of refactoring some old layouts have more than one name and they need to be adjusted accordingly\n",
    "    \"layout_name\": layout, \n",
    "    \"data_path\": CLEAN_2019_HUMAN_DATA_TRAIN,\n",
    "    \"epochs\": 10,\n",
    "    \"old_dynamics\": True,\n",
    "}\n",
    "\n",
    "bc_params = get_bc_params(**params_to_override)\n",
    "train_bc_model(\"tutorial_notebook_results/BC\", bc_params, verbose = True)"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[2], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m layout \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcramped_room\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;66;03m# any compatible layouts \u001B[39;00m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhuman_aware_rl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimitation\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbehavior_cloning_tf2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_bc_params, train_bc_model\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhuman_aware_rl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mstatic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m CLEAN_2019_HUMAN_DATA_TRAIN\n\u001B[1;32m      5\u001B[0m params_to_override \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m      6\u001B[0m     \u001B[38;5;66;03m# this is the layouts where the training will happen\u001B[39;00m\n\u001B[1;32m      7\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mlayouts\u001B[39m\u001B[38;5;124m\"\u001B[39m: [layout], \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mold_dynamics\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     14\u001B[0m }\n",
      "File \u001B[0;32m~/Documents/Afstuderen/src/human_aware_rl/imitation/behavior_cloning_tf2.py:6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mpickle\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mray\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrllib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpolicy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Policy \u001B[38;5;28;01mas\u001B[39;00m RllibPolicy\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keras\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "dc068ebc",
   "metadata": {},
   "source": [
    "# 1): Loading trained agents\n",
    "This section will show you how to load a pretrained agents. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2a9df6",
   "metadata": {},
   "source": [
    "## 1.1) Loading BC agent\n",
    "The BC (behavior cloning) agents are trained separately without using Ray. We showed how to train a BC agent in the previous section, and to load a trained agent, we can use the load_bc_model function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f94ab2a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<keras.engine.functional.Functional at 0x7f73ac2c2110>,\n",
       " {'eager': True,\n",
       "  'use_lstm': False,\n",
       "  'cell_size': 256,\n",
       "  'data_params': {'layouts': ['cramped_room'],\n",
       "   'check_trajectories': False,\n",
       "   'featurize_states': True,\n",
       "   'data_path': '/nas/ucb/micah/overcooked_ai/src/human_aware_rl/static/human_data/cleaned/2019_hh_trials_train.pickle'},\n",
       "  'mdp_params': {'layout_name': 'cramped_room', 'old_dynamics': True},\n",
       "  'env_params': {'horizon': 400,\n",
       "   'mlam_params': {'start_orientations': False,\n",
       "    'wait_allowed': False,\n",
       "    'counter_goals': [],\n",
       "    'counter_drop': [],\n",
       "    'counter_pickup': [],\n",
       "    'same_motion_goals': True}},\n",
       "  'mdp_fn_params': {},\n",
       "  'mlp_params': {'num_layers': 2, 'net_arch': [64, 64]},\n",
       "  'training_params': {'epochs': 10,\n",
       "   'validation_split': 0.15,\n",
       "   'batch_size': 64,\n",
       "   'learning_rate': 0.001,\n",
       "   'use_class_weights': False},\n",
       "  'evaluation_params': {'ep_length': 400, 'num_games': 1, 'display': False},\n",
       "  'action_shape': (6,),\n",
       "  'observation_shape': (96,)})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from human_aware_rl.imitation.behavior_cloning_tf2 import load_bc_model\n",
    "#this is the same path you used when training the BC agent\n",
    "bc_model_path = \"tutorial_notebook_results/BC\"\n",
    "bc_model, bc_params = load_bc_model(bc_model_path)\n",
    "bc_model, bc_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20526ac6",
   "metadata": {},
   "source": [
    "Now that we have loaded the model, since we used Tensorflow to train the agent, we need to wrap it so it is compatible with other agents. We can do it by converting it to a Rllib-compatible policy class, and wraps it as a RllibAgent. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68c37a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<human_aware_rl.rllib.rllib.RlLibAgent at 0x7f73ac5b4040>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from human_aware_rl.imitation.behavior_cloning_tf2 import _get_base_ae, BehaviorCloningPolicy\n",
    "bc_policy = BehaviorCloningPolicy.from_model(bc_model, bc_params, stochastic=True)\n",
    "# We need the featurization function that is specifically defined for BC agent\n",
    "# The easiest way to do it is to create a base environment from the configuration and extract the featurization function\n",
    "# The environment is also needed to do evaluation\n",
    "\n",
    "base_ae = _get_base_ae(bc_params)\n",
    "base_env = base_ae.env\n",
    "\n",
    "from human_aware_rl.rllib.rllib import RlLibAgent\n",
    "bc_agent0 = RlLibAgent(bc_policy, 0, base_env.featurize_state_mdp)\n",
    "bc_agent0\n",
    "\n",
    "bc_agent1 = RlLibAgent(bc_policy, 1, base_env.featurize_state_mdp)\n",
    "bc_agent1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351c5687",
   "metadata": {},
   "source": [
    "Now we have a BC agent that is ready for evaluation "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73698e65",
   "metadata": {},
   "source": [
    "## 1.3) Loading & Creating Agent Pair\n",
    "\n",
    "To do evaluation, we need a pair of agents, or an AgentPair. We can directly load a pair of agents for evaluation, which we can do with the load_agent_pair function, or we can create an AgentPair manually from 2 separate RllibAgent instance. To directly load an AgentPair from a trainer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd83bc",
   "metadata": {},
   "source": [
    "To create an AgentPair manually, we can just pair together any 2 RllibAgent object. For example, we have created a **ppo_agent** and a **bc_agent**. To pair them up, we can just construct an AgentPair with them as arguments."
   ]
  },
  {
   "cell_type": "code",
   "id": "f0acdeee",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T12:28:05.708177Z",
     "start_time": "2025-04-08T12:28:05.649599Z"
    }
   },
   "source": [
    "from human_aware_rl.rllib.rllib import AgentPair\n",
    "ap_bc = AgentPair(bc_agent0, bc_agent1)\n",
    "ap_bc"
   ],
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mhuman_aware_rl\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrllib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mrllib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m AgentPair\n\u001B[1;32m      2\u001B[0m ap_bc \u001B[38;5;241m=\u001B[39m AgentPair(bc_agent0, bc_agent1)\n\u001B[1;32m      3\u001B[0m ap_bc\n",
      "File \u001B[0;32m~/Documents/Afstuderen/src/human_aware_rl/rllib/rllib.py:10\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpathlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mdill\u001B[39;00m\n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mgym\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mgymnasium\u001B[39;00m  \u001B[38;5;66;03m# Things break with gymnasium because rllib can't handle it\u001B[39;00m\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mray\u001B[39;00m\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'gym'"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "4dc6cafa",
   "metadata": {},
   "source": [
    "# 2): Evaluating AgentPair\n",
    "\n",
    "To evaluate an AgentPair, we need to first create an AgentEvaluator. You can create an AgentEvaluator in various ways, but the simpliest way to do so is from the layout_name. \n",
    "\n",
    "You can modify the settings of the layout by changing the **mdp_params** argument, but most of the time you should only need to include \"layout_name\", which is the layout you want to evaluate the agent pair on, and \"old_dynamics\", which determines whether the envrionment conforms to the design in the Neurips2019 paper, or whether the cooking should start automatically when all ingredients are present.  \n",
    "\n",
    "For the **env_params**, you can change how many steps are there in one evaluation. The default is 400, which means the game runs for 400 timesteps. "
   ]
  },
  {
   "cell_type": "code",
   "id": "95787dc6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T12:27:36.786622Z",
     "start_time": "2025-04-08T12:27:36.731506Z"
    }
   },
   "source": [
    "from overcooked_ai_py.agents.benchmarking import AgentEvaluator\n",
    "# Here we create an evaluator for the cramped_room layout\n",
    "layout = \"cramped_room\"\n",
    "ae = AgentEvaluator.from_layout_name(mdp_params={\"layout_name\": layout, \"old_dynamics\": True}, \n",
    "                                     env_params={\"horizon\": 400})\n",
    "ae"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<overcooked_ai_py.agents.benchmarking.AgentEvaluator at 0x333b4f950>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "4471aeda",
   "metadata": {},
   "source": [
    "To run evaluations, we can use the evaluate_agent_pair method associated with the AgentEvaluator:"
   ]
  },
  {
   "cell_type": "code",
   "id": "93676beb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T12:27:42.096378Z",
     "start_time": "2025-04-08T12:27:42.088335Z"
    }
   },
   "source": [
    "# ap: The AgentPair we created earlier\n",
    "# 10: how many times we should run the evaluation since the policy is stochastic\n",
    "trajs = ae.evaluate_agent_pair(ap_bc, 10)\n",
    "trajs"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ap_bc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 3\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;66;03m# ap: The AgentPair we created earlier\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \u001B[38;5;66;03m# 10: how many times we should run the evaluation since the policy is stochastic\u001B[39;00m\n\u001B[0;32m----> 3\u001B[0m trajs \u001B[38;5;241m=\u001B[39m ae\u001B[38;5;241m.\u001B[39mevaluate_agent_pair(\u001B[43map_bc\u001B[49m, \u001B[38;5;241m10\u001B[39m)\n\u001B[1;32m      4\u001B[0m trajs\n",
      "\u001B[0;31mNameError\u001B[0m: name 'ap_bc' is not defined"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "332b6cca",
   "metadata": {},
   "source": [
    "The result returned by the AgentEvaluator contains detailed information about the evaluation runs, including actions taken by each agent at each timestep. Usually you don't need to directly interact with them, but the most direct performance measures can be retrieved with result[\"ep_returns\"], which returns the average sparse reward of each evaluation run"
   ]
  },
  {
   "cell_type": "code",
   "id": "9fed7df5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T14:10:39.803047Z",
     "start_time": "2025-04-04T14:10:39.800098Z"
    }
   },
   "source": [
    "trajs[\"ep_returns\"]"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "48875a68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-04T14:10:41.352289Z",
     "start_time": "2025-04-04T14:10:41.344494Z"
    }
   },
   "source": [
    "result = ae.evaluate_agent_pair(ap_sp, 1, 400)"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ap_sp' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m result \u001B[38;5;241m=\u001B[39m ae\u001B[38;5;241m.\u001B[39mevaluate_agent_pair(\u001B[43map_sp\u001B[49m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m400\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'ap_sp' is not defined"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "4898bae8",
   "metadata": {},
   "source": [
    "# 3): Visualization\n",
    "\n",
    "We can also visualize the trajectories of agents. One way is to run the web demo with the agents you choose, and the specific instructions can be found in the [overcooked_demo](https://github.com/HumanCompatibleAI/overcooked_ai/tree/master/src/overcooked_demo) module, which requires some setup. Another simpler way is to use the StateVisualizer, which uses the information returned by the AgentEvaluator to create a simple dynamic visualization. You can checkout [this Colab Notebook](https://colab.research.google.com/drive/1AAVP2P-QQhbx6WTOnIG54NXLXFbO7y6n#scrollTo=6Xlu54MkiXCR) that let you play with fixed agents"
   ]
  },
  {
   "cell_type": "code",
   "id": "464d0c84",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-08T12:28:20.038302Z",
     "start_time": "2025-04-08T12:28:15.011582Z"
    }
   },
   "source": [
    "from overcooked_ai_py.visualization.state_visualizer import StateVisualizer\n",
    "StateVisualizer().display_rendered_trajectory(trajs, ipython_display=True)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "interactive(children=(IntSlider(value=0, description='timestep', max=399), Output()), _dom_classes=('widget-in…"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fcf96b737a05497a8075ad946bbdd48e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "49b62122",
   "metadata": {},
   "source": [
    "This should spawn a window where you can see what the agents are doing at each timestep. You can drag the slider to go forward and backward in time."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
